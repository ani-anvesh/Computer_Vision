{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "sNqAU4m_Oh8n",
      "metadata": {
        "id": "sNqAU4m_Oh8n"
      },
      "source": [
        "<h1 style=\"font-size:30px;\">Implementing a CNN in PyTorch</h1>\n",
        "\n",
        "In this notebook, weâ€™ll learn how to implement a Convolutional Neural Network (CNN) from scratch using PyTorch. Here, we show a custom CNN architecture with fewer layers. We will learn how to model this architecture and train it on a small dataset called [10 Monkey Species](https://www.kaggle.com/slothkong/10-monkey-species).\n",
        "\n",
        "\n",
        "<center><img src='https://www.dropbox.com/scl/fi/e4541jejdlxzny3vrgw24/Monkey_architecture-updated.png?rlkey=4c3jm0kgzwm4txn1mbewqbox9&st=vq1lb2nt&dl=1' ></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ADOid9KcRfeh",
      "metadata": {
        "id": "ADOid9KcRfeh"
      },
      "source": [
        "## Table of Content\n",
        "\n",
        "* [1. Download and Extract the Dataset](#1.-Download-and-Extract-the-Dataset)\n",
        "* [2. Dataset and Training Configuration Parameters](#2.-Dataset-and-Training-Configuration-Parameters)\n",
        "* [3. Dataset Preprocessing](#3.-Dataset-Preprocessing)\n",
        "* [4. CNN Model Implementation in PyTorch](#4-CNN-Model-Implementation-in-PyTorch)\n",
        "* [5. Model Training and Evaluation](#5.-Model-Training-and-Evaluation)\n",
        "* [6. Saving and Loading Best Model Weights](#6.-Saving-and-Loading-Best-Model-Weights)\n",
        "* [7. Inference](#7.-Inference)\n",
        "* [8. Conclusion](#8.-Conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZYAedU6XOqgB",
      "metadata": {
        "id": "ZYAedU6XOqgB"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd282343-414f-4ec2-8829-d6be20d0ab58",
      "metadata": {
        "id": "fd282343-414f-4ec2-8829-d6be20d0ab58"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchinfo\n",
        "!pip install -q torchvision\n",
        "!pip install -q scikit-learn seaborn\n",
        "!pip install -q tensorboard\n",
        "!pip install -q tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "141c5c29-982a-4ef3-9488-fb70ef4a40de",
      "metadata": {
        "id": "141c5c29-982a-4ef3-9488-fb70ef4a40de"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchinfo import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2 as transforms\n",
        "from torchvision.ops import Conv2dNormActivation\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c0833d-0bd5-4fe7-b205-c9b7073336a2",
      "metadata": {
        "id": "c2c0833d-0bd5-4fe7-b205-c9b7073336a2"
      },
      "outputs": [],
      "source": [
        "#Set seed for reproducibilty\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "       torch.cuda.manual_seed(seed)\n",
        "       torch.cuda.manual_seed_all(seed)\n",
        "       torch.backends.cudnn.deterministic = True\n",
        "       torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Bh2QQnUOtbN",
      "metadata": {
        "id": "3Bh2QQnUOtbN"
      },
      "source": [
        "## 1. Download and Extract the Dataset\n",
        "\n",
        "<center><img src='https://www.dropbox.com/scl/fi/m5kngoab3d9p6ajbffwr9/Monkey-Species-Dataset.jpg?rlkey=t62unc200y259ltk5kqlxa9cj&st=334vllym&dl=1' width=500 height=500></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71a8434b-0b53-4e3c-8063-f6e2f8a5ccbc",
      "metadata": {
        "id": "71a8434b-0b53-4e3c-8063-f6e2f8a5ccbc"
      },
      "outputs": [],
      "source": [
        "!wget -q \"https://www.dropbox.com/s/45jdd8padeyjq6t/10_Monkey_Species.zip?dl=1\" -O \"10_Monkey_Species.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b564d16-3581-4c58-aab0-d2909dac0928",
      "metadata": {
        "id": "0b564d16-3581-4c58-aab0-d2909dac0928"
      },
      "outputs": [],
      "source": [
        "!unzip -q \"10_Monkey_Species.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2joNKZD0OveB",
      "metadata": {
        "id": "2joNKZD0OveB"
      },
      "source": [
        "## 2. Dataset and Training Configuration Parameters\n",
        "\n",
        "Before we describe the model implementation and training, weâ€™re going to apply a little more structure to our training process by using the `dataclasses` module in python to create simple `DatasetConfig` and `TrainingConfig` classes to organize several data and training configuration parameters. This allows us to create data structures for configuration parameters, as shown below. The benefit of doing this is that we have a single place to go to make any desired changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e980d5-16c0-49ae-84ab-a7d9d8a81d1d",
      "metadata": {
        "id": "99e980d5-16c0-49ae-84ab-a7d9d8a81d1d",
        "outputId": "461ff179-dac7-48f0-d039-cfc8f4547c8f"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "      ''' Configuration for Training '''\n",
        "      batch_size: int = 32\n",
        "      num_epochs: int = 100\n",
        "      learning_rate: float = 1e-4\n",
        "\n",
        "      log_interval: int = 1\n",
        "      test_interval: int = 1\n",
        "      data_root: int = \"./\"\n",
        "      num_workers: int = 5\n",
        "      device: str = \"cuda\"\n",
        "\n",
        "train_config = TrainingConfig()\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Available Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YhEj8eNpO2Ed",
      "metadata": {
        "id": "YhEj8eNpO2Ed"
      },
      "source": [
        "<h3 style=\"font-size:30px;\">2.1. Load Custom Datasets in PyTorch</h3>\n",
        "\n",
        "Till now, we have experimented with datasets like Fashion MNIST available with the PyTorch Torchvision library.\n",
        "\n",
        "In this notebook, we will see how to load raw images present in a folder.\n",
        "\n",
        "In the real world, we have need the manage the structure and preprocessing of the dataset on our own.\n",
        "\n",
        "To illustrate a few preprocessing, we have chosen the [10 Monkey Species](https://www.kaggle.com/slothkong/10-monkey-species) dataset from Kaggle. You can download the data from [here](https://www.kaggle.com/slothkong/10-monkey-species/download). You need to extract data. We have already uploaded the extracted data in the lab.\n",
        "\n",
        "Each folder contains 10 subforders labeled as n0~n9, each corresponding a species form Wikipedia's monkey cladogram. Images are 400x300 px or larger and JPEG format (almost 1400 images). Images were downloaded with help of the googliser open source code.\n",
        "\n",
        "**Label mapping:**\n",
        "\n",
        "| Label | Monkey Species |\n",
        "| --- | --- |\n",
        "| n0 | alouatta_palliata |\n",
        "| n1 | erythrocebus_patas |\n",
        "| n2 | cacajao_calvus |\n",
        "| n3 | macaca_fuscata |  \n",
        "| n4 | cebuella_pygmea |\n",
        "| n5 | cebus_capucinus |\n",
        "| n6 | mico_argentatus |\n",
        "| n7 | saimiri_sciureus |\n",
        "| n8 | aotus_nigriceps |\n",
        "| n9 | trachypithecus_johnii |\n",
        "\n",
        "\n",
        "**Extracted Folder structure:**\n",
        "\n",
        "```\n",
        "data_root\n",
        "â”œâ”€â”€ training\n",
        "â”‚Â Â  â””â”€â”€ training\n",
        "â”‚Â Â      â”œâ”€â”€ n0\n",
        "â”‚Â Â      â”œâ”€â”€ n1\n",
        "â”‚Â Â      â”œâ”€â”€ n2\n",
        "â”‚Â Â      â”œâ”€â”€ n3\n",
        "â”‚Â Â      â”œâ”€â”€ n4\n",
        "â”‚Â Â      â”œâ”€â”€ n5\n",
        "â”‚Â Â      â”œâ”€â”€ n6\n",
        "â”‚Â Â      â”œâ”€â”€ n7\n",
        "â”‚Â Â      â”œâ”€â”€ n8\n",
        "â”‚Â Â      â””â”€â”€ n9\n",
        "â””â”€â”€ validation\n",
        "    â””â”€â”€ validation\n",
        "        â”œâ”€â”€ n0\n",
        "        â”œâ”€â”€ n1\n",
        "        â”œâ”€â”€ n2\n",
        "        â”œâ”€â”€ n3\n",
        "        â”œâ”€â”€ n4\n",
        "        â”œâ”€â”€ n5\n",
        "        â”œâ”€â”€ n6\n",
        "        â”œâ”€â”€ n7\n",
        "        â”œâ”€â”€ n8\n",
        "        â””â”€â”€ n9\n",
        "\n",
        "```\n",
        "\n",
        "`data_root/training/training` has `n0-n9` folders; each folder has images of the corresponding class. Similarly, `data_root/validation/validation` has `n0-n9` folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d939cf-1e1b-406a-ac53-5c624898e343",
      "metadata": {
        "id": "93d939cf-1e1b-406a-ac53-5c624898e343"
      },
      "outputs": [],
      "source": [
        "train_root = os.path.join(\"10_Monkey_Species\", \"training\", \"training\")\n",
        "val_root = os.path.join(train_config.data_root, \"10_Monkey_Species\", \"validation\", \"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06adfdba-0dc0-4256-9ef1-0b2989388c72",
      "metadata": {
        "id": "06adfdba-0dc0-4256-9ef1-0b2989388c72",
        "outputId": "d7372648-248e-42d8-a9e0-3d02d7f0424a"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(\"10_Monkey_Species\",\"monkey_labels.txt\"), sep=\",\", header=None)\n",
        "df.columns = [\"Label\", \"Latin Name\", \"Common Name\", \"Train Images\", \"Validation Images\"]\n",
        "df['Latin Name'] = df['Latin Name'].str.replace(\"\\t\", \" \")\n",
        "df[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tAEGtr0hO7L-",
      "metadata": {
        "id": "tAEGtr0hO7L-"
      },
      "source": [
        "## 3. Dataset Preprocessing\n",
        "\n",
        "Here, we normalize the image data to the range `[0,1]` with `mean` and `std` of our Dataset. This is very common when working with image data which helps the model train more efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2be19eb-c1cb-4338-8275-e458f60b6e34",
      "metadata": {
        "id": "b2be19eb-c1cb-4338-8275-e458f60b6e34"
      },
      "outputs": [],
      "source": [
        "mean = [0.4368, 0.4336, 0.3294]  #mean and std of this Monkey Species dataset\n",
        "std = [0.2457, 0.2413, 0.2447]\n",
        "\n",
        "\n",
        "img_size = (224,224)\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(img_size, antialias=True),\n",
        "        transforms.ToTensor()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1960fca-d981-4b16-98a1-3cc6dd8a890a",
      "metadata": {
        "id": "c1960fca-d981-4b16-98a1-3cc6dd8a890a"
      },
      "outputs": [],
      "source": [
        "common_transforms = transforms.Compose(\n",
        "    [\n",
        "        preprocess,\n",
        "        transforms.Normalize(mean=mean,std=std)\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        preprocess,\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "\n",
        "        transforms.RandomErasing(p=0.4),\n",
        "        transforms.RandomApply([\n",
        "        transforms.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
        "         ], p =0.1),\n",
        "\n",
        "        transforms.Normalize(mean = mean,std = std)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4CGd_nMbQD-X",
      "metadata": {
        "id": "4CGd_nMbQD-X"
      },
      "source": [
        "\n",
        "### 3.1. DataLoader Preparation\n",
        "\n",
        "\n",
        "\n",
        "**PyTorch has inbuilt functionality (`torchvision.datasets.ImageFolder` class) to load such structured image folders:**\n",
        "\n",
        "```\n",
        "torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=<function default_loader>, is_valid_file=None)\n",
        "```\n",
        "\n",
        "This is a generic data loader where the images are arranged in this way:\n",
        "\n",
        "```\n",
        "root/n0/xxx.png\n",
        "root/n0/xxy.jpg\n",
        "root/n0/xxz.png\n",
        "\n",
        "root/n1/123.jpg\n",
        "root/n1/nsdf3.png\n",
        "root/n1/asd932_.png\n",
        "\n",
        "    :\n",
        "    :\n",
        "    \n",
        "root/n9/1b23.jpg\n",
        "root/n9/nsasdf3.png\n",
        "root/n9/as2wdd932_.png\n",
        "    \n",
        "```\n",
        "\n",
        "Parameters:\n",
        "\n",
        "- `root` (string) - Root directory path.\n",
        "\n",
        "- `transform` (callable, optional) - A function/transform that takes in an PIL image and returns a transformed version. E.g, `transforms.RandomCrop`.\n",
        "\n",
        "- `target_transform` (callable, optional) - A function/transform that takes in the target and transforms it.\n",
        "\n",
        "- `loader` (callable, optional) - A function to load an image given its path.\n",
        "\n",
        "- `is_valid_file` - A function that takes path of an Image file and check if the file is a valid file (used to check of corrupt files).\n",
        "\n",
        "Find more details [here](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb97980f-6b99-4d11-ab45-beab3d9a46e6",
      "metadata": {
        "id": "bb97980f-6b99-4d11-ab45-beab3d9a46e6",
        "outputId": "cd436190-2b95-46de-b377-ef5f54651786"
      },
      "outputs": [],
      "source": [
        "#Apply augmentations to the training dataset\n",
        "train_data = datasets.ImageFolder(root = train_root, transform = train_transforms)\n",
        "\n",
        "# The validation dataset should have only common transforms like Resize, ToTensor and Normalize.\n",
        "val_data = datasets.ImageFolder(root=val_root, transform = common_transforms)\n",
        "train_data.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23bfebda-0e34-4ab4-88e1-dedb0df01733",
      "metadata": {
        "id": "23bfebda-0e34-4ab4-88e1-dedb0df01733",
        "outputId": "2aff0356-ba85-4485-c653-7eff702e04ce"
      },
      "outputs": [],
      "source": [
        "train_data.class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f45151-38de-43ae-ad86-f5940f6cb280",
      "metadata": {
        "id": "35f45151-38de-43ae-ad86-f5940f6cb280",
        "outputId": "0af6b417-72be-44a7-a3bb-1d0a4a1a8d85"
      },
      "outputs": [],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k4N-xRgTlD2O",
      "metadata": {
        "id": "k4N-xRgTlD2O"
      },
      "source": [
        "**Uncomment the following code block if you would like to find the mean and std of the dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa145623-da5d-49d0-926a-6aff8bac5a69",
      "metadata": {
        "id": "aa145623-da5d-49d0-926a-6aff8bac5a69"
      },
      "outputs": [],
      "source": [
        "# def get_mean_std(train_loader, img_size=(224, 224), num_workers=2):\n",
        "\n",
        "#     batch_mean = torch.zeros(3)\n",
        "#     batch_mean_sqrd = torch.zeros(3)\n",
        "\n",
        "#     for batch_data, _ in train_loader:\n",
        "#         batch_mean += batch_data.mean(dim=(0, 2, 3))  # E[batch_i]\n",
        "#         batch_mean_sqrd += (batch_data**2).mean(dim=(0, 2, 3))  #  E[batch_i**2]\n",
        "\n",
        "#     mean = batch_mean / len(train_loader)\n",
        "\n",
        "#     var = (batch_mean_sqrd / len(train_loader)) - (mean**2)\n",
        "\n",
        "#     std = var**0.5\n",
        "#     print(\"mean: {}, std: {}\".format(mean, std))\n",
        "\n",
        "#     return mean, std\n",
        "\n",
        "# train_data_mean_calc = datasets.ImageFolder(root=train_root, transform = preprocess)\n",
        "# train_loader_mean_calc = DataLoader(train_data_mean_calc, shuffle = True, batch_size = training_config.batch_size, num_workers = train_config.num_workers)\n",
        "# mean, std = get_mean_std(train_loader_mean_calc)\n",
        "\n",
        "# print(\"Mean of Dataset: \", mean)\n",
        "# print(\"Std of Dataset: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de8970dd-8c27-4543-9851-083dee57fb45",
      "metadata": {
        "id": "de8970dd-8c27-4543-9851-083dee57fb45"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    shuffle = True,\n",
        "    batch_size = train_config.batch_size,\n",
        "    num_workers = train_config.num_workers\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_data,\n",
        "    shuffle = False,\n",
        "    batch_size = train_config.batch_size,\n",
        "    num_workers = train_config.num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a56475d-1bc0-45ed-90dd-5bb64d8026d3",
      "metadata": {
        "id": "2a56475d-1bc0-45ed-90dd-5bb64d8026d3",
        "outputId": "417c6e10-cfbf-4373-e0ab-9ed7551ac053"
      },
      "outputs": [],
      "source": [
        "len(next(iter(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deca9104-99d7-4407-a8b1-de533f1d262d",
      "metadata": {
        "id": "deca9104-99d7-4407-a8b1-de533f1d262d"
      },
      "outputs": [],
      "source": [
        "class_mapping = {\n",
        "\n",
        "    0: \"mantled_howler\",\n",
        "    1: \"patas_monkey\",\n",
        "    2: \"bald_uakari\",\n",
        "    3: \"japanese_macaque\",\n",
        "    4: \"pygmy_marmoset\",\n",
        "    5: \"white_headed_capuchin\",\n",
        "    6: \"silvery_marmoset\",\n",
        "    7: \"common_squirrel_monkey\",\n",
        "    8: \"black_headed_night_monkey\",\n",
        "    9: \"nilgiri_langur\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18B55RNQO-a-",
      "metadata": {
        "id": "18B55RNQO-a-"
      },
      "source": [
        "### 3.2. Display Sample Images from the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3417e1-5b77-4262-972a-9aaefbfdd2bd",
      "metadata": {
        "id": "cd3417e1-5b77-4262-972a-9aaefbfdd2bd",
        "outputId": "6600f37c-8fd0-447e-e674-511e688f62ec"
      },
      "outputs": [],
      "source": [
        "def visualize_images(dataloader, num_images = 20):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "\n",
        "    #Iterate over the first batch\n",
        "    images, labels = next(iter(dataloader))\n",
        "    # print(images.shape)\n",
        "\n",
        "    num_rows = 4\n",
        "    num_cols = int(np.ceil((num_images / num_rows)))\n",
        "\n",
        "    for idx in range(min(num_images, len(images))):\n",
        "        image, label = images[idx], labels[idx]\n",
        "\n",
        "\n",
        "        ax = fig.add_subplot(num_rows, num_cols, idx+1, xticks = [], yticks = [])\n",
        "\n",
        "        image = image.permute(1,2,0)\n",
        "\n",
        "        #Normalize the image to [0,1] to display\n",
        "\n",
        "        image = (image - image.min()) / (image.max() - image.min())\n",
        "        ax.imshow(image, cmap=\"gray\")  # remove the batch dimension\n",
        "        ax.set_title(f\"{label.item()}: {class_mapping[label.item()]}\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_images(train_loader, num_images = 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "342aa515-db9c-4c83-a7ec-ea0e3277e333",
      "metadata": {
        "id": "342aa515-db9c-4c83-a7ec-ea0e3277e333"
      },
      "source": [
        "## 4. CNN Model Implementation in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fjHpvvFbPEc3",
      "metadata": {
        "id": "fjHpvvFbPEc3"
      },
      "source": [
        "### 4.1. Define the Convolutional Blocks\n",
        "Letâ€™s start with the very first convolutional layer in the first convolutional block. To define a convolutional layer in PyTorch, we call the `nn.Conv2D()` function, which accepts several input arguments. First, we define the layer to have `32` filters. The kernel size for first filter is `5` and the subsequent layers filter is `3` (which is interpreted as `3x3`). We can use a padding option called `same`, which will pad the input tensor so that the output of the convolution operation has the same spatial size as the input. This is not required, but itâ€™s commonly used. if you donâ€™t explicitly specify this padding option, then the default behavior has no padding, and therefore, the spatial size of output from the convolutional layer will be slightly smaller than the input size. After each convolutional layer, we add a `BatchNorm2d` layer, which normalizes the activations of the previous layer at each batch, thereby improving the training speed and stability of the network. We use a `ReLU`  activation function in all the layers in the Network except for the output layer. This sequence of `Conv2d` followed by `BatchNorm2d` and `ReLU` is called `Conv2dNormActivation` and torchvision has a convenience function to implement this by [`torchvision.ops.Conv2dNormActivation()`](https://pytorch.org/vision/main/generated/torchvision.ops.Conv2dNormActivation.html#conv2dnormactivation).\n",
        "\n",
        "\n",
        "There is also another alternative approach to specify `Conv2d` layers with [`nn.LazyConv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.LazyConv2d.html#lazyconv2d) where the `input_channels` is automatically inferred from the previous `Conv2d` layers `output_channels`.\n",
        "\n",
        "The first two convolutional layers has `32` filters each, and then we follow that with a max pooling layer that has a window size of `(2x2)`,  so the output shape from this first convolution block is `(218 x 218 x 32)`. Next, we have the second convolutional block, has `64` and `128` filters in each convolutional layer instead of `32`, and then finally, the third and fourth convolutional block has `256` and `512` filters.\n",
        "\n",
        "**Note**: The number of filters in each convolutional layer is something that you will need to experiment with. A larger number of filters allows the model to have a greater learning capacity, but this also needs to be balanced with the amount of data available to train the model. Adding too many filters (or layers) can lead to overfitting, one of the most common issues encountered when training models.\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://www.dropbox.com/scl/fi/u3o12g52hbqkgexd2h89l/CNN-Architecure-PyTorch.png?rlkey=yqk1pwobiy82aan4qslv0kb5y&st=az71d8ti&dl=1'  align='center'>\n",
        "\n",
        "\n",
        "The final layer in the feature extractor is the [`nn.AdaptiveAvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#adaptiveavgpool2d), which applies a 2D adaptive average pooling over an input image composed of several input channels. This layer ensures that the output has a fixed size of H x W, regardless of the input size. The number of output features is equal to the number of input channels. This is particularly useful for making the subsequent fully connected layers agnostic to the input size.\n",
        "\n",
        "Before we define the fully connected layers for the classifier, we need to first flatten the two-dimensional activation maps that are produced by the last convolutional layer (which have a spatial shape of `3x3` with `512` channels). This is accomplished by calling the `nn.Flatten()` function to create a 1-dimensional vector of length `4608`. We then add a densely connected layer with `256` neurons and a fully connected output layer with `10` neurons because we have ten classes in our dataset.\n",
        "\n",
        "We will show the different ways in which a `Conv2d` -> `BatchNorm` -> `ReLU`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c723df49-2fe7-4aea-8af2-f25caa4fe1c2",
      "metadata": {
        "id": "c723df49-2fe7-4aea-8af2-f25caa4fe1c2"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self._model = nn.Sequential(\n",
        "\n",
        "        #---------------------- Convolution Layers ----------------------\n",
        "\n",
        "        #-----------------------------------------------\n",
        "        # Conv2d Norm Activation Block1: 32 Filters, MaxPool.\n",
        "        #-----------------------------------------------\n",
        "        nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 5),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(inplace = True),\n",
        "\n",
        "        nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.MaxPool2d(kernel_size = 2),\n",
        "\n",
        "        #-------------------------------------------------------------------\n",
        "        # Conv2d Norm Activation Block 2: 64,128 Filters, MaxPool, Dropout(p=0.25)\n",
        "        #-------------------------------------------------------------------\n",
        "        nn.LazyConv2d(out_channels = 64, kernel_size = 3),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(inplace = True),\n",
        "\n",
        "        nn.LazyConv2d(out_channels = 128, kernel_size = 3),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.MaxPool2d(kernel_size = 2),\n",
        "\n",
        "        #--------------------------------------------------\n",
        "        # Conv2d Norm Activation Block 3: 128,256,512 Filters, MaxPool.\n",
        "        #--------------------------------------------------\n",
        "        Conv2dNormActivation(in_channels = 128, out_channels=256, kernel_size = 3),\n",
        "\n",
        "        Conv2dNormActivation(in_channels = 256, out_channels=256, kernel_size = 3),\n",
        "        nn.MaxPool2d(kernel_size = 2),\n",
        "\n",
        "        Conv2dNormActivation(in_channels = 256, out_channels=512, kernel_size = 3),\n",
        "        nn.MaxPool2d(kernel_size = 2),\n",
        "\n",
        "        #---------------------- Feed Forward Layers --------------------\n",
        "        nn.AdaptiveAvgPool2d(output_size=(3,3)),\n",
        "\n",
        "        #------------------------------------\n",
        "        # Flatten the convolutional features.\n",
        "        #------------------------------------\n",
        "        nn.Flatten(),\n",
        "\n",
        "        #--------------------\n",
        "        # Classification Head.\n",
        "        #--------------------\n",
        "        nn.Linear(in_features = 512*3*3, out_features = 256),\n",
        "        nn.Linear(in_features = 256, out_features = 10)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "      return self._model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f0ef7d-3a98-465a-87a0-aa392a21cd66",
      "metadata": {
        "id": "80f0ef7d-3a98-465a-87a0-aa392a21cd66",
        "outputId": "b7e570f2-f785-45d9-a79a-bd29fabeeb7d"
      },
      "outputs": [],
      "source": [
        "model = MyModel()\n",
        "\n",
        "optimizer  = Adam(model.parameters(), lr = train_config.learning_rate)\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "logdir = \"runs/80epochs-3.3M_param_dropout\"\n",
        "\n",
        "writer = SummaryWriter(logdir)\n",
        "\n",
        "dummy_input = (1,3,224,224)\n",
        "print(summary(model, dummy_input, row_settings = [\"var_names\"],device=\"cpu\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SBxci7E45q2f",
      "metadata": {
        "id": "SBxci7E45q2f"
      },
      "source": [
        "## 5. Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cebdf2b3-7a18-46a4-a175-0fc55a32e3b1",
      "metadata": {
        "id": "cebdf2b3-7a18-46a4-a175-0fc55a32e3b1"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader):\n",
        "    model.train()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    running_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, dim=1)\n",
        "        total_train_samples += labels.shape[0]\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    train_avg_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_predictions / total_train_samples\n",
        "    return train_avg_loss, train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d13dc56-b57e-4e6b-96b0-6a8770289d52",
      "metadata": {
        "id": "1d13dc56-b57e-4e6b-96b0-6a8770289d52"
      },
      "outputs": [],
      "source": [
        "def validation(model, val_loader):\n",
        "    model.eval()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    running_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "             outputs = model(images)\n",
        "\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, dim=1)\n",
        "        total_val_samples += labels.shape[0]\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    val_avg_loss = running_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct_predictions / total_val_samples\n",
        "    return val_avg_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c5af95-4309-40d5-be71-dc7c1a339225",
      "metadata": {
        "id": "b8c5af95-4309-40d5-be71-dc7c1a339225"
      },
      "outputs": [],
      "source": [
        "def main(model, train_loader, val_loader):\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_weights = None\n",
        "\n",
        "    for epoch in range(train_config.num_epochs):\n",
        "        train_loss, train_accuracy = train(model, train_loader)\n",
        "        val_loss, val_accuracy = validation(model, val_loader)\n",
        "\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:0>2}/{train_config.num_epochs} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% - Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Logging metrics to tensorboard\n",
        "        writer.add_scalar('Loss/train', train_loss)\n",
        "        writer.add_scalar('Loss/val', val_loss)\n",
        "        writer.add_scalar('Accuracy/train', train_accuracy)\n",
        "        writer.add_scalar('Accuracy/val', val_accuracy)\n",
        "\n",
        "        if val_accuracy > best_val_acc:\n",
        "            best_val_acc = val_accuracy\n",
        "            best_weights =  model.state_dict()\n",
        "            print(f\"Saving best model...ðŸ’¾\")\n",
        "            torch.save(best_weights, \"best.pt\")\n",
        "\n",
        "    return train_losses, train_accuracies, val_losses, val_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a74e234e-bc4c-4714-b0da-90bb7433e462",
      "metadata": {
        "id": "a74e234e-bc4c-4714-b0da-90bb7433e462",
        "outputId": "5da6ffbf-8b09-473f-80c3-435beafdf7be",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_losses, train_accuracies, val_losses, val_accuracies = main(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747HlqMksWD5",
      "metadata": {
        "id": "747HlqMksWD5"
      },
      "source": [
        "We got best accuracy of **86.76%** val accuracy after training for 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5de005-012b-40ff-9269-4e5c2d05b9e1",
      "metadata": {
        "id": "4e5de005-012b-40ff-9269-4e5c2d05b9e1",
        "outputId": "742fc436-45c0-465e-f3e9-178257287f3d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(1,train_config.num_epochs + 1), train_losses, label = \"Train Loss\")\n",
        "plt.plot(range(1, train_config.num_epochs + 1), val_losses, label = \"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(1,train_config.num_epochs + 1), train_accuracies, label = \"Train Accuracy\")\n",
        "plt.plot(range(1, train_config.num_epochs + 1), val_accuracies, label = \"Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99c278d3-ee49-439b-b5f1-56e3fac5991f",
      "metadata": {
        "id": "99c278d3-ee49-439b-b5f1-56e3fac5991f"
      },
      "source": [
        "### 6. Saving and Loading Best Model Weights\n",
        "Saving and loading best model weights are very convenient with `torch.save()`. This enables you to develop and train a model, save it to the file system and then load it with `torch.load()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccaf4c28-6178-4047-bcba-d8b089405ba5",
      "metadata": {
        "id": "ccaf4c28-6178-4047-bcba-d8b089405ba5",
        "outputId": "dc3bd745-01a5-4414-c691-cf7243383617"
      },
      "outputs": [],
      "source": [
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load(\"best.pt\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jdai3H_RT4uV",
      "metadata": {
        "id": "Jdai3H_RT4uV"
      },
      "source": [
        "## 7. Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b21fb25-b309-482e-8fb6-953dbcd76217",
      "metadata": {
        "id": "2b21fb25-b309-482e-8fb6-953dbcd76217"
      },
      "outputs": [],
      "source": [
        "def prediction(model, val_loader):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    all_images, all_labels = [], []\n",
        "    all_pred_indices, all_pred_probs = [], []\n",
        "\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "             outputs = model(images)\n",
        "\n",
        "        prob = F.softmax(outputs,dim=1)\n",
        "        pred_indices = prob.data.max(dim=1)[1]\n",
        "        pred_probs = prob.data.max(dim=1)[0]\n",
        "\n",
        "        all_images.append(images.cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "        all_pred_indices.append(pred_indices.cpu())\n",
        "        all_pred_probs.append(pred_probs.cpu())\n",
        "\n",
        "\n",
        "    return (torch.cat(all_images).numpy(),\n",
        "            torch.cat(all_labels).numpy(),\n",
        "            torch.cat(all_pred_indices).numpy(),\n",
        "            torch.cat(all_pred_probs).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SfpDqSXMT9we",
      "metadata": {
        "id": "SfpDqSXMT9we"
      },
      "source": [
        "We will need to denormalize our pixel values values by multiplying std and adding mean value to our image for ***Matplotlib visualization***. `np.clip` makes the pixel values to be in range of `[0,1]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7984ba50-f475-45ec-a59c-fb0a8f7255d2",
      "metadata": {
        "id": "7984ba50-f475-45ec-a59c-fb0a8f7255d2"
      },
      "outputs": [],
      "source": [
        "def denormalize(image):\n",
        "    mean_ar = np.array(mean)\n",
        "    std_ar = np.array(std)\n",
        "    image = image * std_ar + mean_ar\n",
        "    return np.clip(image, 0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b219c017-7a1f-4ab7-b825-7e98fc513430",
      "metadata": {
        "id": "b219c017-7a1f-4ab7-b825-7e98fc513430"
      },
      "outputs": [],
      "source": [
        "def visualise_predictions(sample_images,sample_gt_labels, pred_indices, pred_probs, num_images =5):\n",
        "\n",
        "    fig = plt.figure(figsize = (20,5))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        idx = random.randint(0, len(sample_images) -1)\n",
        "        image = sample_images[idx].transpose(1,2,0) #(C,H,W) --> (H,W,C)\n",
        "        label = sample_gt_labels[idx]\n",
        "        pred_idx = pred_indices[idx]\n",
        "        pred_prob = pred_probs[idx]\n",
        "\n",
        "        image = denormalize(image)\n",
        "\n",
        "        ax = fig.add_subplot(1, num_images, i+1)\n",
        "        ax.imshow(image)\n",
        "        ax.set_title(f\"GT: {class_mapping[label]}\\nPred: {class_mapping[pred_idx]} ({pred_prob:.2f})\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf1b040-effc-4753-b9c9-a89de6534e7e",
      "metadata": {
        "id": "bcf1b040-effc-4753-b9c9-a89de6534e7e",
        "outputId": "0c11f367-b38c-48f0-bfc6-ec6fcd642ac4"
      },
      "outputs": [],
      "source": [
        "val_images, val_gt_labels, pred_indices, pred_probs = prediction(model, val_loader)\n",
        "\n",
        "visualise_predictions(val_images, val_gt_labels, pred_indices, pred_probs, num_images = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7304390",
      "metadata": {
        "id": "d7304390"
      },
      "source": [
        "### **7.1. Confusion  matrix**\n",
        "\n",
        "A confusion matrix is a very common metric that is used to summarize the results of a classification problem. The information is presented in the form of a table or matrix where one axis represents the ground truth labels for each class, and the other axis represents the predicted lables from the network. The entries in the table represent the number of instances from an experiment (which are sometimes represented as percentages rather than counts). Generating a confusion matrix is accomplished by calling the `confusion_matrix` from `sklearn.metrics`, which takes two required arguments which are the list of ground truth labels and the associated predicted lables."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V61A6xC0nhLc",
      "metadata": {
        "id": "V61A6xC0nhLc"
      },
      "source": [
        "It can be very informative to better understand where the model performs well and where it may have more difficulty.Â  For example, a few things stand out right away. The model does a very good job in identifying different classes even though there is probable chance of misclassifying, as it is a same species dataset. So all of these observations make intuitive sense, given the similarity of the classes involved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6ff276",
      "metadata": {
        "id": "2a6ff276",
        "outputId": "3f00c99e-2216-4f88-f299-6dd172a47f98"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true=val_gt_labels, y_pred = pred_indices)\n",
        "\n",
        "plt.figure(figsize= [10,5])\n",
        "sn.heatmap(cm, annot=True, fmt='d', annot_kws={\"size\":14})\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Targets\")\n",
        "plt.title(f\"Confusion Matrix\", color=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-_7bIGbhPViO",
      "metadata": {
        "id": "-_7bIGbhPViO"
      },
      "source": [
        "## 8. Conclusion\n",
        "\n",
        "In this notebook, we learned how to use PyTorch to define and train a simple convolutional neural network. We learned how to use `BatchNorm2d` layers as a regularization technique to improve the model's performance on the validation dataset. We also covered how to save and load models to and from the file system. Finally, we evaluated the model's training with a confusion matrix."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
